{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import dataloader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import networks\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics as skl_metrics\n",
    "from typing import List\n",
    "!pip install imbalanced-learn\n",
    "import imblearn\n",
    "\n",
    "\n",
    "logging = dataloader.logging\n",
    "\n",
    "\n",
    "def dice_loss(input, target):\n",
    "    \"\"\"Function to compute dice loss\n",
    "    source: https://github.com/pytorch/pytorch/issues/1249#issuecomment-305088398\n",
    "\n",
    "    Args:\n",
    "        input (torch.Tensor): predictions\n",
    "        target (torch.Tensor): ground truth mask\n",
    "\n",
    "    Returns:\n",
    "        dice loss: 1 - dice coefficient\n",
    "    \"\"\"\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "def make_development_splits(\n",
    "    train_set: pandas.DataFrame,\n",
    "    save_path: Path,\n",
    "    n_folds: int = 5,\n",
    "):\n",
    "    \"\"\"Function to split your training set into 5 folds at a patient-level\n",
    "\n",
    "    Args:\n",
    "        train_set (pandas.DataFrame): pandas dataframe that contains list of nodules\n",
    "        save_path (Path): path to save the splits\n",
    "        n_folds (int, optional): number of folds. Defaults to 5.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(2023)\n",
    "\n",
    "    save_path = Path(save_path)\n",
    "    save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    pids = train_set.patientid.unique()\n",
    "    labs = [train_set[train_set.patientid == pid].malignancy.values[0] for pid in pids]\n",
    "    labs = np.array(labs)\n",
    "\n",
    "    assert len(pids) == len(labs)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    skf.get_n_splits(pids, labs)\n",
    "\n",
    "    folds_missing = False\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "\n",
    "        train_pd = save_path / f\"train{idx}.csv\"\n",
    "        valid_pd = save_path / f\"valid{idx}.csv\"\n",
    "\n",
    "        if not train_pd.is_file():\n",
    "            folds_missing = True\n",
    "\n",
    "        if not valid_pd.is_file():\n",
    "            folds_missing = True\n",
    "\n",
    "    if folds_missing:\n",
    "\n",
    "        print(f\"Making {n_folds} folds from the train set\")\n",
    "\n",
    "        for idx, (train_index, test_index) in enumerate(skf.split(pids, labs)):\n",
    "\n",
    "            train_pids, valid_pids = pids[train_index], pids[test_index]\n",
    "\n",
    "            # split train data into train set and validation set\n",
    "            train_pd = train_set[train_set.patientid.isin(train_pids)]\n",
    "            valid_pd = train_set[train_set.patientid.isin(valid_pids)]\n",
    "\n",
    "            # oversampling to tackle class imbalance\n",
    "            # Strategry: oversample on nodule type, then oversample on malignancy\n",
    "\n",
    "            # resets the index so you don't have gaps between indices\n",
    "            train_pd = train_pd.reset_index(drop=True)\n",
    "            valid_pd = valid_pd.reset_index(drop=True)\n",
    "\n",
    "            train_pd.to_csv(save_path / f\"train{idx}.csv\", index=False)\n",
    "            valid_pd.to_csv(save_path / f\"valid{idx}.csv\", index=False)\n",
    "\n",
    "\n",
    "class NoduleAnalyzer:\n",
    "    \"\"\"Class to train a multi-task nodule analyzer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        best_metric_fn,\n",
    "        workspace: Path,\n",
    "        experiment_id: int,\n",
    "        fold: int = 0,\n",
    "        batch_size: int = 4,\n",
    "        num_workers: int = 16,\n",
    "        max_epochs: int = 1000,\n",
    "        tasks: List = [\n",
    "            \"segmentation\",\n",
    "            \"malignancy\",\n",
    "            \"noduletype\",\n",
    "        ],\n",
    "    ) -> None:\n",
    "\n",
    "        self.workspace = workspace\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.size_mm = 50\n",
    "        self.size_px = 64\n",
    "        self.patch_size = np.array([64, 128, 128])\n",
    "        self.max_rotation_degree = 20\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.learning_rate = 1e-4\n",
    "\n",
    "        self.best_metric_fn = best_metric_fn\n",
    "\n",
    "        date = datetime.today().strftime(\"%Y%m%d\")\n",
    "        self.exp_id = f\"{date}_{experiment_id}\"\n",
    "\n",
    "        self.fold = fold\n",
    "        self.tasks = tasks\n",
    "\n",
    "        train_df_path = workspace / \"data\" / \"luna23-ismi-train-set.csv\"\n",
    "        make_development_splits(\n",
    "            train_set=pandas.read_csv(train_df_path),\n",
    "            save_path=workspace / \"data\" / \"train_set\" / \"folds\",\n",
    "        )\n",
    "\n",
    "        self.train_df = pandas.read_csv(\n",
    "            workspace / \"data\" / \"train_set\" / \"folds\" / f\"train{fold}.csv\"\n",
    "        )\n",
    "        self.valid_df = pandas.read_csv(\n",
    "            workspace / \"data\" / \"train_set\" / \"folds\" / f\"valid{fold}.csv\"\n",
    "        )\n",
    "\n",
    "    def _initialize_model(self, model):\n",
    "\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        # https://stackoverflow.com/questions/58961768/set-torch-backends-cudnn-benchmark-true-or-not\n",
    "\n",
    "        # define the GPU - ideally this is the first GPU, hence cuda:0\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "\n",
    "        # transfer model to GPU\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # define the optimzer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "        )\n",
    "\n",
    "    def _initialize_data_loaders(self):\n",
    "\n",
    "        if \"malignancy\" in self.tasks:\n",
    "            x = self.train_df.malignancy.values\n",
    "            x = dataloader.make_weights_for_balanced_classes(x)\n",
    "            weights = x\n",
    "\n",
    "        if \"noduletype\" in self.tasks:\n",
    "            y = self.train_df.noduletype.values\n",
    "            y = [dataloader.NODULETYPE_MAPPING[t] for t in y]\n",
    "            y = dataloader.make_weights_for_balanced_classes(y)\n",
    "            weights = y\n",
    "\n",
    "        if \"malignancy\" in self.tasks and \"noduletype\" in self.tasks:\n",
    "            weights = x * y  # ðŸ¥š Easter egg\n",
    "\n",
    "        if \"malignancy\" in self.tasks or \"noduletype\" in self.tasks:\n",
    "            weights = torch.DoubleTensor(weights)\n",
    "            sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "                weights,\n",
    "                len(self.train_df),\n",
    "            )\n",
    "\n",
    "        if self.tasks == [\"segmentation\"]:\n",
    "            sampler = None\n",
    "\n",
    "        self.train_loader = dataloader.get_data_loader(\n",
    "            self.workspace / \"data\" / \"train_set\",\n",
    "            self.train_df,\n",
    "            sampler=sampler,\n",
    "            workers=self.num_workers // 2,\n",
    "            batch_size=self.batch_size,\n",
    "            rotations=[(-self.max_rotation_degree, self.max_rotation_degree)] * 3,\n",
    "            translations=True,\n",
    "            size_mm=self.size_mm,\n",
    "            size_px=self.size_px,\n",
    "            patch_size=self.patch_size,\n",
    "        )\n",
    "\n",
    "        self.valid_loader = dataloader.get_data_loader(\n",
    "            self.workspace / \"data\" / \"train_set\",\n",
    "            self.valid_df,\n",
    "            workers=self.num_workers // 2,\n",
    "            batch_size=self.batch_size,\n",
    "            size_mm=self.size_mm,\n",
    "            size_px=self.size_px,\n",
    "            patch_size=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_data, update_weights=False):\n",
    "\n",
    "        images, masks, noduletype_targets, malignancy_targets = (\n",
    "            batch_data[\"image\"].to(self.device),\n",
    "            batch_data[\"mask\"].to(self.device),\n",
    "            batch_data[\"noduletype_target\"].to(self.device),\n",
    "            batch_data[\"malignancy_target\"].to(self.device),\n",
    "        )\n",
    "\n",
    "        targets, losses = {}, {}\n",
    "        loss = 0  # ðŸ¥š Easter egg\n",
    "\n",
    "        if update_weights:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        outputs = self.model(images)  # do the forward pass\n",
    "\n",
    "        if \"malignancy\" in self.tasks:\n",
    "\n",
    "            malignancy_loss = F.binary_cross_entropy(\n",
    "                outputs[\"malignancy\"],\n",
    "                malignancy_targets,\n",
    "            )\n",
    "\n",
    "            losses[\"malignancy\"] = malignancy_loss.item()\n",
    "            outputs[\"malignancy\"] = outputs[\"malignancy\"].data.cpu().numpy().reshape(-1)\n",
    "            targets[\"malignancy\"] = malignancy_targets.data.cpu().numpy().reshape(-1)\n",
    "\n",
    "            loss += malignancy_loss\n",
    "\n",
    "        if \"noduletype\" in self.tasks:\n",
    "\n",
    "            noduletype_loss = F.cross_entropy(\n",
    "                outputs[\"noduletype\"],\n",
    "                noduletype_targets.squeeze().long(),\n",
    "            )\n",
    "\n",
    "            losses[\"noduletype\"] = noduletype_loss.item()\n",
    "            outputs[\"noduletype\"] = (\n",
    "                outputs[\"noduletype\"].data.cpu().numpy().reshape(-1, 4)\n",
    "            )\n",
    "            targets[\"noduletype\"] = noduletype_targets.data.cpu().numpy().reshape(-1)\n",
    "\n",
    "            loss += noduletype_loss\n",
    "\n",
    "        if \"segmentation\" in self.tasks:\n",
    "\n",
    "            segmentation_loss = dice_loss(\n",
    "                outputs[\"segmentation\"],\n",
    "                masks,\n",
    "            )\n",
    "\n",
    "            losses[\"segmentation\"] = segmentation_loss.item()\n",
    "            outputs[\"segmentation\"] = outputs[\"segmentation\"]\n",
    "            targets[\"segmentation\"] = masks.data.cpu().numpy()\n",
    "\n",
    "            loss += segmentation_loss\n",
    "\n",
    "        losses[\"total\"] = loss.item()\n",
    "\n",
    "        if update_weights:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return outputs, targets, losses\n",
    "\n",
    "    def train(self, model):\n",
    "\n",
    "        self._initialize_data_loaders()\n",
    "        self._initialize_model(model)\n",
    "\n",
    "        save_dir = self.workspace / \"results\" / self.exp_id / f\"fold{self.fold}\"\n",
    "        save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        epoch_metrics = {\n",
    "            \"training\": [],\n",
    "            \"validation\": [],\n",
    "        }\n",
    "\n",
    "        best_metric = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "\n",
    "            for mode in [\"training\", \"validation\"]:\n",
    "\n",
    "                print(\"\\n\\n\")\n",
    "                logging.info(\n",
    "                    f\" {mode}, epoch: {epoch + 1} / {self.max_epochs}, with fold: {self.fold}\"\n",
    "                )\n",
    "                logging.info(f\"Tasks being trained: {self.tasks}\")\n",
    "\n",
    "                if mode == \"training\":\n",
    "                    self.model.train()\n",
    "                    data = self.train_loader\n",
    "                else:\n",
    "                    self.model.eval()\n",
    "                    data = self.valid_loader\n",
    "\n",
    "                metrics = {\n",
    "                    task: {\n",
    "                        \"loss\": [],\n",
    "                    }\n",
    "                    for task in self.tasks\n",
    "                }\n",
    "                metrics[\"cumulative\"] = {\"loss\": []}\n",
    "\n",
    "                predictions = {task: [] for task in self.tasks}\n",
    "                labels = {task: [] for task in self.tasks}\n",
    "\n",
    "                for batch_data in tqdm(data):\n",
    "\n",
    "                    if mode == \"training\":\n",
    "\n",
    "                        outputs, targets, losses = self.forward(\n",
    "                            batch_data,\n",
    "                            update_weights=True,\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            outputs, targets, losses = self.forward(\n",
    "                                batch_data,\n",
    "                                update_weights=False,\n",
    "                            )\n",
    "\n",
    "                    for task in self.tasks:\n",
    "                        metrics[task][\"loss\"].append(losses[task])\n",
    "\n",
    "                    metrics[\"cumulative\"][\"loss\"].append(losses[\"total\"])\n",
    "\n",
    "                    for task in self.tasks:\n",
    "                        predictions[task].extend(outputs[task])\n",
    "                        labels[task].extend(targets[task])\n",
    "\n",
    "                for task in self.tasks:\n",
    "                    loss = np.mean(metrics[task][\"loss\"])\n",
    "                    metrics[task][\"loss\"] = loss  # aggregate the loss\n",
    "\n",
    "                metrics[\"cumulative\"][\"loss\"] = np.mean(metrics[\"cumulative\"][\"loss\"])\n",
    "\n",
    "                if \"malignancy\" in self.tasks:\n",
    "\n",
    "                    x = predictions[\"malignancy\"]\n",
    "                    y = labels[\"malignancy\"]\n",
    "                    metrics[\"malignancy\"][\"auc\"] = skl_metrics.roc_auc_score(y, x)\n",
    "\n",
    "                if \"noduletype\" in self.tasks:\n",
    "\n",
    "                    x = [p.argmax() for p in predictions[\"noduletype\"]]\n",
    "                    y = labels[\"noduletype\"]\n",
    "\n",
    "                    metrics[\"noduletype\"][\n",
    "                        \"balanced_accuracy\"\n",
    "                    ] = skl_metrics.balanced_accuracy_score(y, x)\n",
    "\n",
    "                if \"segmentation\" in self.tasks:\n",
    "\n",
    "                    dice = 1 - np.mean(metrics[\"segmentation\"][\"loss\"])\n",
    "                    metrics[\"segmentation\"][\"dice\"] = dice\n",
    "\n",
    "                epoch_metrics[mode].append(metrics)\n",
    "\n",
    "                if mode == \"validation\":\n",
    "\n",
    "                    if self.best_metric_fn(metrics) > best_metric:\n",
    "\n",
    "                        print(\"\\n===== Saving best model! =====\\n\")\n",
    "                        best_metric = self.best_metric_fn(metrics)\n",
    "                        best_epoch = epoch\n",
    "                        torch.save(\n",
    "                            self.model.state_dict(),\n",
    "                            save_dir / \"best_model.pth\",\n",
    "                        )\n",
    "                        np.save(save_dir / \"best_metrics.npy\", metrics)\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        print(f\"Model has not improved since epoch {best_epoch + 1}\")\n",
    "\n",
    "                metrics = pandas.DataFrame(metrics).round(3)\n",
    "                metrics.replace(np.nan, \"\", inplace=True)\n",
    "                print(metrics.to_markdown(tablefmt=\"grid\"))\n",
    "\n",
    "            np.save(save_dir / \"metrics.npy\", epoch_metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    workspace = Path(\"/code/bodyct-luna23-ismi-trainer/\")\n",
    "\n",
    "    def best_metric_fn(metrics):\n",
    "        return metrics[\"segmentation\"][\"dice\"]  # ðŸ¥š Easter egg\n",
    "\n",
    "    ## uncomment the following block for the classification tasks\n",
    "    # model = networks.CNN3D(\n",
    "    #     n_input_channels=1,\n",
    "    #     n_output_channels=1,  # set output channels to 4 for noduletype classification\n",
    "    #     task=\"malignancy\",\n",
    "    # )\n",
    "\n",
    "    model = networks.UNet(1, n_filters=64)\n",
    "\n",
    "    nodule_analyzer = NoduleAnalyzer(\n",
    "        workspace=workspace,\n",
    "        best_metric_fn=best_metric_fn,\n",
    "        experiment_id=\"0_segmentation\",  # give your experiment a unique ID, for each run\n",
    "        batch_size=4,  # increase batch size to 32 for the classification tasks\n",
    "        fold=0,  # ðŸ¥š Easter egg\n",
    "        max_epochs=100,  # set max epochs to 1000 for the classification tasks\n",
    "        tasks=[\"segmentation\"],  # ðŸ¥š Easter egg\n",
    "    )\n",
    "    nodule_analyzer.train(model)  # ðŸ¥š Easter egg"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
