{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import dataloader_duplicate\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import multitask_network\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics as skl_metrics\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = dataloader_duplicate.logging\n",
    "project_dir = \"C:/Users/eline/OneDrive/Documenten/LUNA23-ISMI-Group7\"\n",
    "\n",
    "def dice_loss(input, target):\n",
    "    \"\"\"Function to compute dice loss\n",
    "    source: https://github.com/pytorch/pytorch/issues/1249#issuecomment-305088398\n",
    "\n",
    "    Args:\n",
    "        input (torch.Tensor): predictions\n",
    "        target (torch.Tensor): ground truth mask\n",
    "\n",
    "    Returns:\n",
    "        dice loss: 1 - dice coefficient\n",
    "    \"\"\"\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "def make_development_splits(\n",
    "    train_set: pandas.DataFrame,\n",
    "    save_path: Path(project_dir+\"splits/\"),\n",
    "    n_folds: int = 5,\n",
    "):\n",
    "    \"\"\"Function to split your training set into 5 folds at a patient-level\n",
    "\n",
    "    Args:\n",
    "        train_set (pandas.DataFrame): pandas dataframe that contains list of nodules\n",
    "        save_path (Path): path to save the splits\n",
    "        n_folds (int, optional): number of folds. Defaults to 5.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(2023)\n",
    "\n",
    "    save_path = Path(save_path)\n",
    "    save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    pids = train_set.patientid.unique()\n",
    "    labs = [train_set[train_set.patientid == pid].malignancy.values[0] for pid in pids]\n",
    "    labs = np.array(labs)\n",
    "\n",
    "    assert len(pids) == len(labs)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    skf.get_n_splits(pids, labs)\n",
    "\n",
    "    folds_missing = False\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "\n",
    "        train_pd = save_path / f\"train{idx}.csv\"\n",
    "        valid_pd = save_path / f\"valid{idx}.csv\"\n",
    "\n",
    "        if not train_pd.is_file():\n",
    "            folds_missing = True\n",
    "\n",
    "        if not valid_pd.is_file():\n",
    "            folds_missing = True\n",
    "\n",
    "    if folds_missing:\n",
    "\n",
    "        print(f\"Making {n_folds} folds from the train set\")\n",
    "\n",
    "        for idx, (train_index, test_index) in enumerate(skf.split(pids, labs)):\n",
    "\n",
    "            train_pids, valid_pids = pids[train_index], pids[test_index]\n",
    "\n",
    "            train_pd = train_set[train_set.patientid.isin(train_pids)]\n",
    "            valid_pd = train_set[train_set.patientid.isin(valid_pids)]\n",
    "\n",
    "            train_pd = train_pd.reset_index(drop=True)\n",
    "            valid_pd = valid_pd.reset_index(drop=True)\n",
    "\n",
    "            train_pd.to_csv(save_path / f\"train{idx}.csv\", index=False)\n",
    "            valid_pd.to_csv(save_path / f\"valid{idx}.csv\", index=False)\n",
    "\n",
    "\n",
    "class NoduleAnalyzer:\n",
    "    \"\"\"Class to train a multi-task nodule analyzer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        best_metric_fn,\n",
    "        workspace: Path(project_dir),\n",
    "        experiment_id: int,\n",
    "        fold: int = 0,\n",
    "        batch_size: int = 4,\n",
    "        num_workers: int = 16,\n",
    "        max_epochs: int = 1000,\n",
    "        tasks: List = [\n",
    "            \"segmentation\",\n",
    "            \"malignancy\",\n",
    "            \"noduletype\",\n",
    "        ],\n",
    "    ) -> None:\n",
    "\n",
    "        self.workspace = workspace\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.size_mm = 50\n",
    "        self.size_px = 64\n",
    "        self.patch_size = np.array([64, 128, 128])\n",
    "        self.max_rotation_degree = 20\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.learning_rate = 1e-4\n",
    "\n",
    "        self.best_metric_fn = best_metric_fn\n",
    "\n",
    "        date = datetime.today().strftime(\"%Y%m%d\")\n",
    "        self.exp_id = f\"{date}_{experiment_id}\"\n",
    "\n",
    "        self.fold = fold\n",
    "        self.tasks = tasks\n",
    "\n",
    "        train_df_path = workspace / \"data\" / \"luna23-ismi-train-set.csv\"\n",
    "        make_development_splits(\n",
    "            train_set=pandas.read_csv(train_df_path),\n",
    "            save_path=workspace / \"data\" / \"train_set\" / \"folds\",\n",
    "        )\n",
    "\n",
    "        self.train_df = pandas.read_csv(\n",
    "            workspace / \"data\" / \"train_set\" / \"folds\" / f\"train{fold}.csv\"\n",
    "        )\n",
    "        self.valid_df = pandas.read_csv(\n",
    "            workspace / \"data\" / \"train_set\" / \"folds\" / f\"valid{fold}.csv\"\n",
    "        )\n",
    "\n",
    "    def _initialize_model(self, model):\n",
    "\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        # https://stackoverflow.com/questions/58961768/set-torch-backends-cudnn-benchmark-true-or-not\n",
    "\n",
    "        # define the GPU - ideally this is the first GPU, hence cuda:0\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "\n",
    "        # transfer model to GPU\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # define the optimzer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "        )\n",
    "\n",
    "    def _initialize_data_loaders(self):\n",
    "\n",
    "        if \"malignancy\" in self.tasks:\n",
    "            x = self.train_df.malignancy.values\n",
    "            x = dataloader_duplicate.make_weights_for_balanced_classes(x)\n",
    "            weights = x\n",
    "\n",
    "        if \"noduletype\" in self.tasks:\n",
    "            y = self.train_df.noduletype.values\n",
    "            y = [dataloader_duplicate.NODULETYPE_MAPPING[t] for t in y]\n",
    "            y = dataloader_duplicate.make_weights_for_balanced_classes(y)\n",
    "            weights = y\n",
    "\n",
    "        if \"malignancy\" in self.tasks and \"noduletype\" in self.tasks:\n",
    "            weights = x * y  # ðŸ¥š Easter egg\n",
    "\n",
    "        if \"malignancy\" in self.tasks or \"noduletype\" in self.tasks:\n",
    "            weights = torch.DoubleTensor(weights)\n",
    "            sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "                weights,\n",
    "                len(self.train_df),\n",
    "            )\n",
    "\n",
    "        if self.tasks == [\"segmentation\"]:\n",
    "            sampler = None\n",
    "\n",
    "        self.train_loader = dataloader_duplicate.get_data_loader(\n",
    "            self.workspace / \"data\" / \"train_set\",\n",
    "            self.train_df,\n",
    "            sampler=sampler,\n",
    "            workers=self.num_workers // 2,\n",
    "            batch_size=self.batch_size,\n",
    "            rotations=[(-self.max_rotation_degree, self.max_rotation_degree)] * 3,\n",
    "            translations=True,\n",
    "            size_mm=self.size_mm,\n",
    "            size_px=self.size_px,\n",
    "            patch_size=self.patch_size,\n",
    "        )\n",
    "\n",
    "        self.valid_loader = dataloader_duplicate.get_data_loader(\n",
    "            self.workspace / \"data\" / \"train_set\",\n",
    "            self.valid_df,\n",
    "            workers=self.num_workers // 2,\n",
    "            batch_size=self.batch_size,\n",
    "            size_mm=self.size_mm,\n",
    "            size_px=self.size_px,\n",
    "            patch_size=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_data, update_weights=False):\n",
    "\n",
    "        images, masks, noduletype_targets, malignancy_targets = (\n",
    "            batch_data[\"image\"].to(self.device),\n",
    "            batch_data[\"mask\"].to(self.device),\n",
    "            batch_data[\"noduletype_target\"].to(self.device),\n",
    "            batch_data[\"malignancy_target\"].to(self.device),\n",
    "        )\n",
    "\n",
    "        targets, losses = {}, {}\n",
    "        loss = 0  # ðŸ¥š Easter egg\n",
    "\n",
    "        if update_weights:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        outputs = self.model(images)  # do the forward pass\n",
    "\n",
    "        if \"malignancy\" in self.tasks:\n",
    "\n",
    "            malignancy_loss = F.binary_cross_entropy(\n",
    "                outputs[\"malignancy\"],\n",
    "                malignancy_targets,\n",
    "            )\n",
    "\n",
    "            losses[\"malignancy\"] = malignancy_loss.item()\n",
    "            outputs[\"malignancy\"] = outputs[\"malignancy\"].data.cpu().numpy().reshape(-1)\n",
    "            targets[\"malignancy\"] = malignancy_targets.data.cpu().numpy().reshape(-1)\n",
    "\n",
    "            loss += malignancy_loss\n",
    "\n",
    "        if \"noduletype\" in self.tasks:\n",
    "\n",
    "            noduletype_loss = F.cross_entropy(\n",
    "                outputs[\"noduletype\"],\n",
    "                noduletype_targets.squeeze().long(),\n",
    "            )\n",
    "\n",
    "            losses[\"noduletype\"] = noduletype_loss.item()\n",
    "            outputs[\"noduletype\"] = (\n",
    "                outputs[\"noduletype\"].data.cpu().numpy().reshape(-1, 4)\n",
    "            )\n",
    "            targets[\"noduletype\"] = noduletype_targets.data.cpu().numpy().reshape(-1)\n",
    "\n",
    "            loss += noduletype_loss\n",
    "\n",
    "        if \"segmentation\" in self.tasks:\n",
    "\n",
    "            segmentation_loss = dice_loss(\n",
    "                outputs[\"segmentation\"],\n",
    "                masks,\n",
    "            )\n",
    "\n",
    "            losses[\"segmentation\"] = segmentation_loss.item()\n",
    "            outputs[\"segmentation\"] = outputs[\"segmentation\"]\n",
    "            targets[\"segmentation\"] = masks.data.cpu().numpy()\n",
    "\n",
    "            loss += segmentation_loss\n",
    "\n",
    "        losses[\"total\"] = loss.item()\n",
    "\n",
    "        if update_weights:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return outputs, targets, losses\n",
    "\n",
    "    def train(self, model):\n",
    "\n",
    "        self._initialize_data_loaders()\n",
    "        self._initialize_model(model)\n",
    "\n",
    "        save_dir = self.workspace / \"results\" / self.exp_id / f\"fold{self.fold}\"\n",
    "        save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        epoch_metrics = {\n",
    "            \"training\": [],\n",
    "            \"validation\": [],\n",
    "        }\n",
    "\n",
    "        best_metric = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "\n",
    "            for mode in [\"training\", \"validation\"]:\n",
    "\n",
    "                print(\"\\n\\n\")\n",
    "                logging.info(\n",
    "                    f\" {mode}, epoch: {epoch + 1} / {self.max_epochs}, with fold: {self.fold}\"\n",
    "                )\n",
    "                logging.info(f\"Tasks being trained: {self.tasks}\")\n",
    "\n",
    "                if mode == \"training\":\n",
    "                    self.model.train()\n",
    "                    data = self.train_loader\n",
    "                else:\n",
    "                    self.model.eval()\n",
    "                    data = self.valid_loader\n",
    "\n",
    "                metrics = {\n",
    "                    task: {\n",
    "                        \"loss\": [],\n",
    "                    }\n",
    "                    for task in self.tasks\n",
    "                }\n",
    "                metrics[\"cumulative\"] = {\"loss\": []}\n",
    "\n",
    "                predictions = {task: [] for task in self.tasks}\n",
    "                labels = {task: [] for task in self.tasks}\n",
    "\n",
    "                for batch_data in tqdm(data):\n",
    "\n",
    "                    if mode == \"training\":\n",
    "\n",
    "                        outputs, targets, losses = self.forward(\n",
    "                            batch_data,\n",
    "                            update_weights=True,\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            outputs, targets, losses = self.forward(\n",
    "                                batch_data,\n",
    "                                update_weights=False,\n",
    "                            )\n",
    "\n",
    "                    for task in self.tasks:\n",
    "                        metrics[task][\"loss\"].append(losses[task])\n",
    "\n",
    "                    metrics[\"cumulative\"][\"loss\"].append(losses[\"total\"])\n",
    "\n",
    "                    for task in self.tasks:\n",
    "                        predictions[task].extend(outputs[task])\n",
    "                        labels[task].extend(targets[task])\n",
    "\n",
    "                for task in self.tasks:\n",
    "                    loss = np.mean(metrics[task][\"loss\"])\n",
    "                    metrics[task][\"loss\"] = loss  # aggregate the loss\n",
    "\n",
    "                metrics[\"cumulative\"][\"loss\"] = np.mean(metrics[\"cumulative\"][\"loss\"])\n",
    "\n",
    "                if \"malignancy\" in self.tasks:\n",
    "\n",
    "                    x = predictions[\"malignancy\"]\n",
    "                    y = labels[\"malignancy\"]\n",
    "                    metrics[\"malignancy\"][\"auc\"] = skl_metrics.roc_auc_score(y, x)\n",
    "\n",
    "                if \"noduletype\" in self.tasks:\n",
    "\n",
    "                    x = [p.argmax() for p in predictions[\"noduletype\"]]\n",
    "                    y = labels[\"noduletype\"]\n",
    "\n",
    "                    metrics[\"noduletype\"][\n",
    "                        \"balanced_accuracy\"\n",
    "                    ] = skl_metrics.balanced_accuracy_score(y, x)\n",
    "\n",
    "                if \"segmentation\" in self.tasks:\n",
    "\n",
    "                    dice = 1 - np.mean(metrics[\"segmentation\"][\"loss\"])\n",
    "                    metrics[\"segmentation\"][\"dice\"] = dice\n",
    "\n",
    "                epoch_metrics[mode].append(metrics)\n",
    "\n",
    "                if mode == \"validation\":\n",
    "\n",
    "                    if self.best_metric_fn(metrics) > best_metric:\n",
    "\n",
    "                        print(\"\\n===== Saving best model! =====\\n\")\n",
    "                        best_metric = self.best_metric_fn(metrics)\n",
    "                        best_epoch = epoch\n",
    "                        torch.save(\n",
    "                            self.model.state_dict(),\n",
    "                            save_dir / \"best_model.pth\",\n",
    "                        )\n",
    "                        np.save(save_dir / \"best_metrics.npy\", metrics)\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        print(f\"Model has not improved since epoch {best_epoch + 1}\")\n",
    "\n",
    "                metrics = pandas.DataFrame(metrics).round(3)\n",
    "                metrics.replace(np.nan, \"\", inplace=True)\n",
    "                print(metrics.to_markdown(tablefmt=\"grid\"))\n",
    "\n",
    "            np.save(save_dir / \"metrics.npy\", epoch_metrics)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = Path(project_dir)\n",
    "\n",
    "def best_metric_fn(metrics):\n",
    "    return metrics[\"segmentation\"][\"dice\"]  # ðŸ¥š Easter egg\n",
    "\n",
    "    ## uncomment the following block for the classification tasks\n",
    "    # model = networks.CNN3D(\n",
    "    #     n_input_channels=1,\n",
    "    #     n_output_channels=1,  # set output channels to 4 for noduletype classification\n",
    "    #     task=\"malignancy\",\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making 5 folds from the train set\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f901b9b23303>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mtasks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"segmentation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# ðŸ¥š Easter egg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         )\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mnodule_analyzer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ðŸ¥š Easter egg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-0cbbf3b0d953>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_data_loaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0msave_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"results\"\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp_id\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34mf\"fold{self.fold}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-0cbbf3b0d953>\u001b[0m in \u001b[0;36m_initialize_model\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# transfer model to GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;31m# define the optimzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[1;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[1;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m                 \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m         raise RuntimeError(\n\u001b[0;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[1;32mfrom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "for i in range(1): #aangepast\n",
    "        model = multitask_network.UNet(1, n_filters=64)\n",
    "        nodule_analyzer = NoduleAnalyzer(\n",
    "            workspace=workspace,\n",
    "            best_metric_fn=best_metric_fn,\n",
    "            experiment_id=\"0_segmentation\",  # give your experiment a unique ID, for each run\n",
    "            batch_size=4,  # increase batch size to 32 for the classification tasks\n",
    "            fold=i,  # ðŸ¥š Easter egg\n",
    "            max_epochs=100,  # set max epochs to 1000 for the classification tasks\n",
    "            tasks=[\"segmentation\"],  # ðŸ¥š Easter egg\n",
    "        )\n",
    "        nodule_analyzer.train(model)  # ðŸ¥š Easter egg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
